[
  {
    "path": "posts/2023-09-29-nycairbnb-data-analytics-case-study/",
    "title": "NYC AirBnB Data Analytics Case Study",
    "description": "Identify a marketplace that connects people renting houses with people looking for a place to stay.",
    "author": [
      {
        "name": "Agus Santoso",
        "url": {}
      }
    ],
    "date": "2023-09-29",
    "categories": [
      "Python",
      "Data Visualizations",
      "Marketing Analysis"
    ],
    "contents": "\nIntroductions\nThis personal project is a case study given by RevoU in a mini course held for 2 weeks to see the participantsâ€™ understanding of the Data Analytics material that has been presented, therefore I tried to make several analyzes of this Airbnb which is a marketplace that connects people who renting out houses to people looking for a place to stay.\nWhat Tools To Use\nGoogle Colaboratory\nPython\nSummary\n\nFig. 1. Te Kahu, Wanaka, New Zealand\nAirbnb is an online marketplace that connects people who want to rent out their homes with people looking for accommodations in specific locales. The company has come a long way since 2007, when its co-founders first came up with the idea to invite paying guests to sleep on an air mattress in their living room. According to Airbnbâ€™s latest data, it now has more than 7 million listings, covering some 100,000 cities and towns in 220-plus countries and regions worldwide.\nExploratory Data Analytics\nPreparation\nGoogle Colaboratory\nImport Libraries Python: pandas, numpy, matplotlib\nThis public dataset is part of Airbnb, This is New York City Airbnb Open Data from Dgomonov.\nConnect with the data\nConnect the data\nThe first step is that we have to make sure our data is connected.\n\n# Connect with dataset in drive\nfrom google.colab import drive\ndrive.mount(\"/content/drive\")\n\nImport and Reading Data\nImport libraries\nImport libraries we need to use, like:\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport random as random\nimport random\nfrom datetime import datetime, timedelta\nimport plotly.express as px\nimport warnings\n\nRead the data\nRead the data\n\n# Read the data\ndf = pd.read_csv(\"/content/drive/MyDrive/dataset nyc airbnb/AB_NYC_2019.csv\")\n\nData Understandings\nBasic data understandings\nThis is basic data understandings like:\nDataFrame : df.\nshape : df.shape()\nhead : df.head()\ninfo : df.info()\ndescribe : df.describe()\nData Processing\nInformations of Dataset\nIn the first step of data processing we can use several database understandings to see an overview of the dataset to data type information and check for the duplicates values.\n\ndf.head() # View the data\ndf.describe() # View the descriptive statistics or overview the dataset\ndf.info() # View the info from data like data type and others\nprint(df.shape) # Provide the number of rows and columns in the dataset\nlist(df) # View list of column names\ndf.drop_duplicates() # Check for the duplicates values\ndf.shape\nprint(df.isnull().sum()) # Print the number of null values in each column\n\nFixing Error\nSecond step is fixing error form the data\nChange the data type if something is wrong\n\n# Change the data type from object to date\ndf['last_review'] = pd.to_datetime(df['last_review'])\ndf['review_year'] = df['last_review'].apply(lambda last_review:last_review.year)\ndf['review_year'] = df['review_year'].fillna(0)\ndf['review_year'] = df.review_year.astype(int)\ndf = pd.concat([df[(df['availability_365']==0) & (df['review_year']==2019)],df[df['availability_365']>0]])\n\nReplace missing values\n\n#Replace the missing value with 0\ndf['reviews_per_month'] = df['reviews_per_month'].fillna(0)\n\nCheck again the data info\n\ndf.info()\n\nFixing again if there is still something you want to fix\nhere i want to drop data last_review and eplace the missing â€˜nameâ€™ values by random name.\n\n# Drop the 'last_review' column\ndf.drop('last_review', axis=1, inplace=True)\n\n# Replace the missing 'name' values by random name\ndf = df.fillna({'name': 'Upper East Side Oasis!'}) # here i fill with 'Upper East Side Oasis!'\ndf = df.fillna({'host_name': 'john'}) # here i fill with 'john'\n\nView the dataset\nview the dataset with df.head() and df.info() and check too dataset isnull.\n\ndf.head()\ndf.info()\nprint(df.isnull().sum())\n\nDesign Visualizations\nHeatmap Correlations\nCorrelation heatmaps are here to show us how closely related variables are.\n\nplt.figure(figsize=(12,12))\nsns.heatmap(df.corr(), cmap='Blues', annot=True,)\nplt.show()\n\nBar plot\nTo shows Proportion of Airbnb listings across boroughs and room type.\n\n# Create a bar plot borough\n(df['neighbourhood_group'].value_counts() / df.shape[0]).plot.bar(cmap='tab10', title='Proportion of Airbnb listings across boroughs')\nplt.xlabel('Borough')\nplt.ylabel('Proportion')\nplt.xticks(rotation=45)\nplt.show()\n\n# Create a bar plot room type\n(df['room_type'].value_counts() / df.shape[0]).plot.bar(cmap='tab10', title='Proportion of Airbnb listings across room_type')\nplt.xlabel('Room Type')\nplt.ylabel('Proportion')\nplt.xticks(rotation=45)\nplt.show()\n\nCount of Room Types by Neighbourhood Group\nTo see room type by neighbourhood group\n\n# Create the count plot\nplt.figure(figsize=(10, 10))\nsns.set(style=\"whitegrid\")\n# Create a count plot with 'room_type' on the x-axis and hue='neighbourhood_group'\nsns.countplot(data=df, x='room_type', hue='neighbourhood_group', palette='viridis')\nplt.title('Count of Room Types by Neighbourhood Group')\n# Show the plot\nplt.show()\n\nMost Popular Neighbourhood Group Based on Availability\nTo see which ones are the most popular based on availability\n\n# Create the bar plot\nplt.figure(figsize=(10, 6))\nsns.set(style=\"whitegrid\")\n# Sort the DataFrame by 'availability_365' in descending order\ndf_sorted = df.sort_values(by='availability_365', ascending=False)\n# Create the bar plot\nsns.boxplot(data=df_sorted, x='neighbourhood_group', y='availability_365', palette='viridis')\nplt.xlabel('Neighbourhood Group')\nplt.ylabel('Availability (in days)')\nplt.title('Most Popular Neighbourhood Group Based on Availability')\n# Show the plot\nplt.show()\n\nMost Expensive\nTo see where is the most expensive area?\n\n# Create the bar plot\nfig = sns.catplot(x='neighbourhood_group', y='price', data=df, kind='bar', hue='room_type', palette='viridis')\n# Add a title and adjust its position\nfig.fig.suptitle('Where is the most expensive area?', fontsize=15, y=1.05)\n# Save the plot as an image with tight layout\nfig.savefig('most_expensive_area.png', bbox_inches='tight')\n\nPopular area?\nTo seewhere is the most popular area?\n\n# Create the bar plot\nfig1 = sns.catplot(x='neighbourhood_group',y='availability_365',kind='box',hue='room_type',data=df,palette='viridis')\n# Add a title and adjust its position\nfig1.fig.suptitle('Where is the most popular area?',fontsize=15,y=1.05)\n# Save the plot as an image with tight layout\nfig1.savefig('most_popular_area.png', bbox_inches = 'tight')\n\nMap Box\nNext, we can visualize it using a box map to see the distribution.\n\n# Visualize Map Box with scatterplot\nplt.figure(figsize=(10,10))\nsns.scatterplot(x='longitude', y='latitude', hue='neighbourhood_group',s=20, data=df, palette=\"viridis\")\n\nTop 5 Host Listing\nTo see who the top 5 by calculated_host_listings_count\n\n# Calculate total reviews per host\ntotal_host_listings_count = df.groupby('host_name')['calculated_host_listings_count'].sum()\n# Rank hosts by total listings count\nranked_hosts = total_host_listings_count.sort_values(ascending=False)\n# Top 5 host_names based on total listings count\ntop_5_host_names = ranked_hosts.head(5)\n# Display the top 5 host_names\nprint(top_5_host_names)\n\n# Create a bar plot to visualize the top 5 host_names by total listings\nplt.figure(figsize=(10, 6))\nplt.bar(top_5_host_names.index, top_5_host_names.values, color='skyblue')\nplt.xlabel('Host Name')\nplt.ylabel('Total Listings')\nplt.title('Top 5 Hosts by Total Listings')\nplt.xticks(rotation=45)\nplt.tight_layout()\n\n# Show the plot\nplt.show()\n\nVisualizations and Analysis\nHeatmaps Correlations\nA correlation heatmap is a graphical representation of a correlation matrix representing the correlation between different variables. The value of correlation can take any value from -1 to 1. Correlation between two random variables or bivariate data does not necessarily imply a causal relationship. We can see it as in Fig. 2 below ðŸ‘‡\n\nFig. 2. Heatmaps Correlations.\n\n\n\n",
    "preview": "posts/2023-09-29-nycairbnb-data-analytics-case-study/luxury_airbnb.webp",
    "last_modified": "2023-10-02T10:02:55+07:00",
    "input_file": "nycairbnb-data-analytics-case-study.knit.md"
  },
  {
    "path": "posts/2023-09-24-timesheet-monitoring-tracker/",
    "title": "TimeSheet Monitoring Tracker",
    "description": "A TimeSheet Monitoring Tracker to track the working hours performed by the intern employees.",
    "author": [
      {
        "name": "Agus Santoso",
        "url": {}
      }
    ],
    "date": "2023-09-24",
    "categories": [
      "Spreadsheets",
      "Monitoring Tracker",
      "Data Visualizations"
    ],
    "contents": "\nIntroductions\nThis project is undertaken due to the people teamâ€™s and mentor need to track the working hours performed by the intern employees during their work activities on a weekly basis. This is to observe the performance patterns of the interns.\nWhat Tools To Use\nSpreadsheets\nSCQA Framework\nHereâ€™s the SCQA Frameworks we use:\nSituation:\nThere is a working hour regulation for interns, which is currently set at 40 hours per week.\nThere is a discussion about changing the working hour regulation with two options: 30 hours per week or 15 hours per week.\nThe Campaign has implemented a work deliverables sheet in the commitment form, which serves as evidence of data regarding the tasks and working hours of interns every day.\nComplication:\nThe implementation of work deliverables has not been optimal due to the difficulty in monitoring, both by mentors and the people team.\nFeedback from some interns indicates that it is not very effective due to excessive administrative work, such as the obligation to fill out standup.ly and Jira tasks.\nAs a result, data related to the average working hours of interns per week cannot be compiled.\nQuestion:\nHow can we monitor and track the working hours of interns along with the tasks/projects they are working on?\nAnswer:\nTime/Task/Project Sheet Tracker\nModify the existing work deliverables sheet to create a sheet/form that is visually easier for interns to fill out.\nMentors can more easily monitor and provide feedback on a weekly/monthly basis.\nWorking hour data can be pulled by the people team, ensuring that interns comply with their choice of working hours (30 or 15 hours per week).\nPreparations\nHere, I want to describe a few tools in this analysis:\nSpreadsheets\nTools/Functions we use:\nFiltering\nSlicer\nImportrange\nFormating\nIF Functions\netc\n\nFiles : I want to know you Data files are hidden due to company secrets.\nStep and Plan\nCreate Template Work Deliverables Sheet\nCreate a new template with modifications from the previous sheet. This template will later be duplicated by each intern, then the data in the work deliverables sheet will be pulled into the Time Sheet Tracker (people-team data center). Heres the picture Work Deliverables Sheet Template.\n\nFig. 1. Template Work Deliverables Sheet.\nTimeSheet Monitoring Tracker\nCreate a TimeSheet Monitoring Tracker (Central Data). In the time sheet tracker there will be 4 sections:\nGuidlines: Contains How to Use the TimeSheet Monitoring Tracker\n\nFig. 2. Guidlines Tracker\nPeople Team Monitoring: Contains a total recap of internal work deliverables such as total working hours per week per intern.\n\nFig. 3. People Team Monitoring Tracker\nMentor Monitoring: Recap of internal work deliverables per team, to make it easier for mentors to check without having to open internal deliverables one by one.\n\nFig. 4. Mentor Monitoring Tracker\nData-Viz: To see a graph of the development of internal working hours.\n\nFig. 5. Data Vizualization Monitoring Tracker\nTrial and Error\nAfter the work deliverables sheets and TimeSheet Monitoring Tracker are created, we carry out the next step, namely by Trial & Error to finding out of the best way to reach a desired result or a correct solution by trying out one or more ways or means and by noting and eliminating errors or causes of failure.\nEvaluations\nEvaluation needs to be carried out to assess or calculate the quality of the TimeSheet Monitoring Tracker that we have developed. We can do this with all the data that has been collected since TimeSheet Monitoring Tracker was used.\nFinalization\nIn the last step we finalize it after all the steps have been carried out and the TimeSheet Monitoring Tracker can be used properly.\n\n\n\n",
    "preview": "posts/2023-09-24-timesheet-monitoring-tracker/mt-peopleteam.png",
    "last_modified": "2023-09-29T15:38:04+07:00",
    "input_file": {},
    "preview_width": 1440,
    "preview_height": 900
  }
]
