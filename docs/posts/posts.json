[
  {
    "path": "posts/2023-09-29-nycairbnb-data-analytics-case-study/",
    "title": "NYC AirBnB Data Analytics Case Study",
    "description": "Identify a marketplace that connects people renting houses with people looking for a place to stay.",
    "author": [
      {
        "name": "Agus Santoso",
        "url": {}
      }
    ],
    "date": "2023-09-29",
    "categories": [
      "Python",
      "Data Visualizations",
      "Marketing Analysis"
    ],
    "contents": "\nIntroductions\nThis personal project is a case study given by RevoU in a mini course held for 2 weeks to see the participantsâ€™ understanding of the Data Analytics material that has been presented, therefore I tried to make several analyzes of this Airbnb which is a marketplace that connects people who renting out houses to people looking for a place to stay.\nWhat Tools To Use\nGoogle Colaboratory\nPython\nSummary\n\nFig. 1. Te Kahu, Wanaka, New Zealand\nAirbnb is an online marketplace that connects people who want to rent out their homes with people looking for accommodations in specific locales. The company has come a long way since 2007, when its co-founders first came up with the idea to invite paying guests to sleep on an air mattress in their living room. According to Airbnbâ€™s latest data, it now has more than 7 million listings, covering some 100,000 cities and towns in 220-plus countries and regions worldwide.\nExploratory Data Analytics\nPreparation\nGoogle Colaboratory\nImport Libraries Python: pandas, numpy, matplotlib\nThis public dataset is part of Airbnb, This is New York City Airbnb Open Data from Dgomonov.\nConnect with the data\nConnect the data\nThe first step is that we have to make sure our data is connected.\n\n# Connect with dataset in drive\nfrom google.colab import drive\ndrive.mount(\"/content/drive\")\n\nImport and Reading Data\nImport libraries\nImport libraries we need to use, like:\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport random as random\nimport random\nfrom datetime import datetime, timedelta\nimport plotly.express as px\nimport warnings\n\nRead the data\nRead the data\n\n# Read the data\ndf = pd.read_csv(\"/content/drive/MyDrive/dataset nyc airbnb/AB_NYC_2019.csv\")\n\nData Understandings\nBasic data understandings\nThis is basic data understandings like:\nDataFrame : df.\nshape : df.shape()\nhead : df.head()\ninfo : df.info()\ndescribe : df.describe()\nData Processing\nInformations of Dataset\nIn the first step of data processing we can use several database understandings to see an overview of the dataset to data type information and check for the duplicates values.\n\ndf.head() # View the data\ndf.describe() # View the descriptive statistics or overview the dataset\ndf.info() # View the info from data like data type and others\nprint(df.shape) # Provide the number of rows and columns in the dataset\nlist(df) # View list of column names\ndf.drop_duplicates() # Check for the duplicates values\ndf.shape\nprint(df.isnull().sum()) # Print the number of null values in each column\n\nFixing Error\nSecond step is fixing error form the data\nChange the data type if something is wrong\n\n# Change the data type from object to date\ndf['last_review'] = pd.to_datetime(df['last_review'])\ndf['review_year'] = df['last_review'].apply(lambda last_review:last_review.year)\ndf['review_year'] = df['review_year'].fillna(0)\ndf['review_year'] = df.review_year.astype(int)\ndf = pd.concat([df[(df['availability_365']==0) & (df['review_year']==2019)],df[df['availability_365']>0]])\n\nReplace missing values\n\n#Replace the missing value with 0\ndf['reviews_per_month'] = df['reviews_per_month'].fillna(0)\n\nCheck again the data info\n\ndf.info()\n\nFixing again if there is still something you want to fix\nhere i want to drop data last_review and eplace the missing â€˜nameâ€™ values by random name.\n\n# Drop the 'last_review' column\ndf.drop('last_review', axis=1, inplace=True)\n\n# Replace the missing 'name' values by random name\ndf = df.fillna({'name': 'Upper East Side Oasis!'}) # here i fill with 'Upper East Side Oasis!'\ndf = df.fillna({'host_name': 'john'}) # here i fill with 'john'\n\nView the dataset\nview the dataset with df.head() and df.info() and check too dataset isnull.\n\ndf.head()\ndf.info()\nprint(df.isnull().sum())\n\nDesign Visualizations\nHeatmap Correlations\nCorrelation heatmaps are here to show us how closely related variables are.\n\nplt.figure(figsize=(12,12))\nsns.heatmap(df.corr(), cmap='Blues', annot=True,)\nplt.show()\n\nCount of Room Types by Neighbourhood Group\nTo see room type by neighbourhood group\n\n# Create the count plot\nplt.figure(figsize=(10, 10))\nsns.set(style=\"whitegrid\")\n# Create a count plot with 'room_type' on the x-axis and hue='neighbourhood_group'\nsns.countplot(data=df, x='room_type', hue='neighbourhood_group', palette='viridis')\nplt.title('Count of Room Types by Neighbourhood Group')\n# Show the plot\nplt.show()\n\nBar plot\nTo shows Proportion of Airbnb listings across boroughs and room type.\n\n# Create a bar plot borough\n(df['neighbourhood_group'].value_counts() / df.shape[0]).plot.bar(cmap='tab10', title='Proportion of Airbnb listings across boroughs')\nplt.xlabel('Borough')\nplt.ylabel('Proportion')\nplt.xticks(rotation=45)\nplt.show()\n\n# Create a bar plot room type\n(df['room_type'].value_counts() / df.shape[0]).plot.bar(cmap='tab10', title='Proportion of Airbnb listings across room_type')\nplt.xlabel('Room Type')\nplt.ylabel('Proportion')\nplt.xticks(rotation=45)\nplt.show()\n\nMost Expensive\nTo see where is the most expensive area?\n\n# Create the bar plot\nfig = sns.catplot(x='neighbourhood_group', y='price', data=df, kind='bar', hue='room_type', palette='viridis')\n# Add a title and adjust its position\nfig.fig.suptitle('Where is the most expensive area?', fontsize=15, y=1.05)\n# Save the plot as an image with tight layout\nfig.savefig('most_expensive_area.png', bbox_inches='tight')\n\nPopular area?\nTo see where is the most popular area based on availability?\n\n# Create the bar plot\nplt.figure(figsize=(10, 6))\nsns.set(style=\"whitegrid\")\n# Sort the DataFrame by 'availability_365' in descending order\ndf_sorted = df.sort_values(by='availability_365', ascending=False)\n# Create the bar plot\nsns.boxplot(data=df_sorted, x='neighbourhood_group', y='availability_365', palette='viridis')\nplt.xlabel('Neighbourhood Group')\nplt.ylabel('Availability (in days)')\nplt.title('Most Popular Neighbourhood Group Based on Availability')\n# Show the plot\nplt.show()\n\nMap Box\nNext, we can visualize it using a box map to see the distribution.\n\n# Visualize Map Box with scatterplot\nplt.figure(figsize=(10,10))\nsns.scatterplot(x='longitude', y='latitude', hue='neighbourhood_group',s=20, data=df, palette=\"viridis\")\n\nTop 5 Host Listing\nTo see who the top 5 by calculated_host_listings_count\n\n# Calculate total reviews per host\ntotal_host_listings_count = df.groupby('host_name')['calculated_host_listings_count'].sum()\n# Rank hosts by total listings count\nranked_hosts = total_host_listings_count.sort_values(ascending=False)\n# Top 5 host_names based on total listings count\ntop_5_host_names = ranked_hosts.head(5)\n# Display the top 5 host_names\nprint(top_5_host_names)\n\n# Create a bar plot to visualize the top 5 host_names by total listings\nplt.figure(figsize=(10, 6))\nplt.bar(top_5_host_names.index, top_5_host_names.values, color='skyblue')\nplt.xlabel('Host Name')\nplt.ylabel('Total Listings')\nplt.title('Top 5 Hosts by Total Listings')\nplt.xticks(rotation=45)\nplt.tight_layout()\n\n# Show the plot\nplt.show()\n\nVisualizations and Analysis\nHeatmaps Correlations\nA correlation heatmap is a graphical representation of a correlation matrix representing the correlation between different variables. The value of correlation can take any value from -1 to 1. Correlation between two random variables or bivariate data does not necessarily imply a causal relationship. We can see it as in Fig. 2 below ðŸ‘‡\nFig. 2. Heatmaps Correlations.Count of Room Types by Neighbourhood Group\nTo create an analysis of this, we first collected data from Airbnb listings and created Airbnb areas based on neighborhood groups such as: Manhattan, Bronx, Brooklyn, Queens, and Staten Island. We then calculated three main room types: Whole house/apartment, Shared room, and Private room in each neighborhood group. We can see the details in Fig. 3.\nFig. 3. Count of Room Types by Neighbourhood GroupThe results indicate that Manhattan has the highest number of Entire home/apt listings, reflecting its popularity as a tourist destination. Brooklyn follows closely behind, offering a mix of room types. In contrast, the Bronx, Queens, and Staten Island have a higher proportion of Entire home/apt and Private rooms, likely due to their residential nature.\nProportion of Airbnb listings across boroughs and room_type\nThis is Breakdown from Count of Room Types by Neighbourhood Group\nProportion of Airbnb listings across boroughs\nIn this analysis, we explore the distribution of Airbnb listings across the five major boroughs of New York City: Manhattan, Bronx, Brooklyn, Queens, and Staten Island. We can see details in Fig. 4.\nFig. 4. Proportion of Airbnb Listings Across Boroughs.Regarding boroughs in this graph Fig. 4., Manhattan and Brooklyn have the highest proportions of Airbnb listings, reflecting their status as top tourist destinations. Queens also boasts a significant number of listings, while the Bronx offers a more affordable alternative. Staten Island, with its suburban appeal, has the smallest share of Airbnb listings among the five boroughs.\nProportion of Airbnb listings across room_type\nIn addition to region, We explored the Airbnb listing distribution of three room types in New York City: Entire home/apt, Private room, and Shared room. We can see details in Fig. 5.\nFig. 5. Proportions of Airbnb Listings Across Room Type.In terms of room types, Airbnb listings in New York City predominantly comprise â€œEntire home/aptâ€ options, catering to those seeking privacy and convenience. â€œPrivate roomâ€ listings come next, offering a balance between affordability and comfort. â€œShared roomâ€ listings are the least common, serving budget-conscious solo travelers or those comfortable sharing living spaces.\nWhere The Most Expensive Area?\nTo analyze the data, we first gathered information on rental prices for these room types in each borough. Afterward, we calculated the average rental price for each combination of borough and room type. Here are the findings:\nManhattan: Unsurprisingly, Manhattan emerges as the most expensive borough overall, with entire homes/apartments being the costliest, followed by private rooms and shared rooms.\nBronx: In the Bronx, shared rooms are the most affordable option, followed by private rooms and entire homes/apartments.\nBrooklyn: Brooklyn exhibits a similar pattern to Manhattan, with entire homes/apartments being the most expensive, followed by private rooms and shared rooms.\nQueens: Queens generally offers more affordable accommodations compared to Manhattan and Brooklyn. Entire homes/apartments are the priciest, followed by private rooms and shared rooms.\nStaten Island: Staten Island, being the least expensive of the five boroughs, sees private rooms as the most economical choice, followed by shared rooms and entire homes/apartments.\nTo visualize these findings, weâ€™ve created bar graphs below that represent the average rental prices for each borough and room type. These graphs will help you better understand the price distribution across the different areas and accommodation types. We can see details in Fig. 6.\nFig. 6. The Most Expensive Area.Please note that the specific rental prices can vary greatly within each borough, and this analysis provides a general overview. If you have access to the relevant data, you can create more detailed analyses and visuals to dive deeper into the specific neighborhoods and factors influencing rental prices in each area.\nMost Popular Neighbourhood Group Based On Availability?\nThe box plot analysis depicts the availability of Airbnb listings in the five major boroughs of New York City (Manhattan, Bronx, Brooklyn, Queens, and Staten Island). The â€˜availability_365â€™ metric is used to measure the availability of listings throughout the year. We can see the details in Fig. 7.\nFig. 7. The Most Popular Area Based On Availability.Looking at the above categorical box plot we can infer that the listings in State Island seems to be more available throughout the year to more than 300 days. On an average, these listings are available to around more 250 days every year followed by Bronx where every listings are available for around more 150 on an average every year.\nLetâ€™s also see Map Box Distributions\nFig. 8. Map Box Distributions.Mapbox distribution in New York City shows varying levels of usage across the five boroughs: Manhattan, Bronx, Brooklyn, Queens, and Staten Island. In Manhattan, where the bustling heart of the city resides, Mapbox is likely to see high utilization, with its mapping and location services catering to the demands of businesses, tourists, and residents alike.\nTop 5 Host Listing\nTo find Top 5 hosts by total listings count.\nFig. 9. Top 5 Host Listings.As we can find that Top 5 host name are Sonder (NYC), Blueground, Kara, Kazuya, and Sonder.\nConclusion\nThe results indicate that Manhattan has the highest number of Entire home/apt listings, reflecting its popularity as a tourist destination. Brooklyn follows closely behind, offering a mix of room types. In contrast, the Bronx, Queens, and Staten Island have a higher proportion of Entire home/apt and Private rooms, likely due to their residential nature.\nWe can infer that there are high range of prices across Manhattan followed by Brooklyn and Queens being the most costliest place to stay in NYC.\nUnderstanding these proportions can assist travelers in selecting accommodations that suit their preferences and budgets.\nObservations\nManhattan has the highest number of Entire home/apt listings, reflecting its popularity as a tourist destination. Brooklyn follows closely behind, offering a mix of room types. Then Manhattan have more expensive places to stay in NYC. Room availability is very low in manhattan and brooklyn and you can find a room anytime in the State Island and Bronx.\nRecomendations\nIf you are looking for the most expensive locations you can come to Manhattan and Brooklyn or Queens. However, if you want to find the places with the highest availability in New York City, you can choose a location like the Bronx with an average of more than 150 days every year or State Island with an average of more than 250 days available every year.\n\nTHANK YOU ðŸ™Œ\n\n\n\n\n",
    "preview": "posts/2023-09-29-nycairbnb-data-analytics-case-study/luxury_airbnb.webp",
    "last_modified": "2023-10-03T11:47:22+07:00",
    "input_file": "nycairbnb-data-analytics-case-study.knit.md"
  },
  {
    "path": "posts/2023-10-03-utilizing-health-tech-device-usage-trends-to-inform-marketing-strategy-bellabeat-analysis/",
    "title": "Utilizing Health Tech Device Usage Trends to Inform Marketing Strategy: Bellabeat Analysis",
    "description": "To analyze smart device data to gain insight into how consumers are using their smart devices.",
    "author": [
      {
        "name": "Agus Santoso",
        "url": {}
      }
    ],
    "date": "2023-09-29",
    "categories": [
      "Python",
      "Data Visualizations",
      "Marketing Analysis"
    ],
    "contents": "\nIntroductions\nScenario\nWelcome to the Bellabeat data analysis case study! In this case study, I will perform many real-world tasks of a junior data analyst. I will imagine I am working for Bellabeat, a high-tech manufacturer of health-focused products for women, and meet different characters and team members. In order to answer the key business questions, you will follow the steps of the data analysis process: ask, prepare, process, analyze, share, and act.\n\nFig. 1. Bellabeat Leafâ€™s.\nI am a junior data analyst working on the marketing analyst team at Bellabeat, a high-tech manufacturer of health-focused products for women. Bellabeat is a successful small company, but they have the potential to become a larger player in the global smart device market. UrÅ¡ka SrÅ¡en, cofounder and Chief Creative Officer of Bellabeat, believes that analyzing smart device fitness data could help unlock new growth opportunities for the company. I have been asked to focus on one of Bellabeatâ€™s products and analyze smart device data to gain insight into how consumers are using their smart devices. The insights I discover will then help guide marketing strategy for the company. I will present your analysis to the Bellabeat executive team along with your high-level recommendations for Bellabeatâ€™s marketing strategy.\nWhat Tools To Use\nGoogle Colaboratory\nPython\nExploratory Data Analytics\nGoogle Colaboratory\nImport Libraries Python: pandas, numpy, matplotlib, etc.\nThis public dataset in here\nConnect with the data\nConnect the data\nThe first step is that we have to make sure our data is connected.\n\n#Connect google colab with my drive\nfrom google.colab import drive\ndrive.mount(\"/content/drive\")\n\nImport and Reading Data\nImport libraries\nImport libraries we need to use, like:\n\n#import libraries all we need\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport random as random\nimport plotly.graph_objects as go\nfrom plotly.offline import iplot\nfrom plotnine.data import economics\nfrom plotnine import ggplot, aes, geom_line\nimport warnings\n\nRead the data\nRead the data\n\n#import the data we use\ndaily_activity  = pd.read_csv(\"/content/drive/MyDrive/Fitabase Dataset/dailyActivity_merged.csv\")\ndaily_calories = pd.read_csv(\"/content/drive/MyDrive/Fitabase Dataset/dailyCalories_merged.csv\")\nsleep_day  = pd.read_csv(\"/content/drive/MyDrive/Fitabase Dataset/sleepDay_merged.csv\")\nweight_log_info  = pd.read_csv(\"/content/drive/MyDrive/Fitabase Dataset/weightLogInfo_merged.csv\")\n\nData Understandings\nBasic data understandings\nThis is basic data understandings like:\nDataFrame : df.\nshape : df.shape()\nhead : df.head()\ninfo : df.info()\ndescribe : df.describe()\nData Processing\nInformations of Dataset\nIn the first step of data processing we can use several database understandings to see an overview of the dataset to data type information and check for the duplicates values.\nBreakdown the data to make analysis easier\nDaily Activity Dataset\n\ndaily_activity.head() # View the data\ndaily_activity.describe() # View the descriptive statistics or overview the dataset\nddaily_activity.info() # View the info from data like data type and others\nprint(daily_activity.shape) # Provide the number of rows and columns in the dataset\nlist(daily_activity) # View list of column names\ndaily_activity.drop_duplicates() # Check for the duplicates values\ndaily_activity.shape\nprint(daily_activity.isnull().sum()) # Print the number of null values in each column\n\nFixing Error\nSecond step is fixing error form the data\nChange the data type if something is wrong\n\nFixing Error from data type ActivityDay â€“> from object to date\n\n#Change the data type from object to date\ndaily_activity['ActivityDate'] = pd.to_datetime(daily_activity['ActivityDate'])\n\nCreate/Adding New Columns\n\n# Create year column\ndaily_activity['Year'] = daily_activity['ActivityDate'].dt.year\n\n# Create month column\ndaily_activity['Month'] = daily_activity['ActivityDate'].dt.month\n\n# Create day column\ndaily_activity['Day'] = daily_activity['ActivityDate'].dt.day\n\n# Create day_of_week column\ndaily_activity['Day_of_week'] = daily_activity['ActivityDate'].dt.day_name()\n\nCheck again the data info\n\ndaily_activity.info()\ndaily_activity.head()\n\ncheck the information after change data type and adding the data with daily_activity.info() and daily_activity.head(). Details like the image below.\n\nFig. 2. Screenshot daily_activity.info()\nNoted: Do the same for daily_calories, sleep_day, and weight_log_info adjusted to the dataset. If all the required datasets have been prepared and processed, you can proceed to the next step.\nDesign Visualizations\nHeatmap Correlations\n\nplt.figure(figsize=(14,14))\nsns.heatmap(daily_activity.corr(), cmap='Greens', annot=True,)\nplt.show()\n\nVery Active Minutes by Day of the Week\n\n# Calculate the total VeryActiveMinutes for each day of the week\nvery_active_minutes_by_day = daily_activity.groupby(\"Day_of_week\")[\"VeryActiveMinutes\"].sum()\n\n# Create a list of days of the week in the desired order\ndays_of_week = [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"]\n\n# Use the tab10 colormap for the bars\ncolors = plt.cm.tab10(range(len(days_of_week)))\n\n# Create a bar chart to visualize the data\nplt.figure(figsize=(10, 6))\nplt.bar(days_of_week, very_active_minutes_by_day, color=colors)\nplt.xlabel(\"Day of the Week\")\nplt.ylabel(\"Very Active Minutes\")\nplt.title(\"Very Active Minutes by Day of the Week\")\nplt.show()\n\nSedentary Minutes by Day of the Week\n\n# Calculate the total SedentaryMinutes for each day of the week\nsedentary_minutes_by_day = daily_activity.groupby(\"Day_of_week\")[\"SedentaryMinutes\"].sum()\n\n# Create a list of days of the week in the desired order\ndays_of_week = [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"]\n\n# Use the tab10 colormap for the bars\ncolors = plt.cm.tab10(range(len(days_of_week)))\n\n# Create a bar chart to visualize the data\nplt.figure(figsize=(10, 6))\nplt.bar(days_of_week, sedentary_minutes_by_day, color=colors)\nplt.xlabel(\"Day of the Week\")\nplt.ylabel(\"Sedentary Minutes\")\nplt.title(\"Sedentary Minutes by Day of the Week\")\nplt.show()\n\nTotal Steps by Day of the Week\n\n# Calculate the total TotalSteps for each day of the week\ntotal_steps_by_day = daily_activity.groupby(\"Day_of_week\")[\"TotalSteps\"].sum()\n\n# Create a list of days of the week in the desired order\ndays_of_week = [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"]\n\n# Create a bar chart to visualize the data\nplt.figure(figsize=(10, 6))\nplt.bar(days_of_week, total_steps_by_day)\nplt.xlabel(\"Day of the Week\")\nplt.ylabel(\"Total Steps\")\nplt.title(\"Total Steps by Day of the Week\")\nplt.show()\n\nRelationship between Total Steps and Calories\n\n# Create a scatter plot using Seaborn\nplt.figure(figsize=(10, 6))\nsns.scatterplot(data=daily_activity, x=\"TotalSteps\", y=\"Calories\", hue=\"SedentaryMinutes\", palette=\"coolwarm\", alpha=0.7)\nplt.xlabel(\"Total Steps\")\nplt.ylabel(\"Calories\")\nplt.title(\"Relationship between Total Steps and Calories\")\nplt.grid(True)\n\n# Add a regression line (similar to geom_smooth in ggplot2)\nsns.regplot(data=daily_activity, x=\"TotalSteps\", y=\"Calories\", scatter=False, color=\"gray\")\n\nplt.show()\n\nTotal Distance During Each day\n\nda_days = daily_activity.groupby('Day_of_week').agg({'TotalDistance':'sum'}).reset_index().sort_values('TotalDistance',ascending = False)\n\na1 = (8, 6)\nfig, ax = plt.subplots(figsize=a1)\nplot= sns.barplot(x=\"Day_of_week\", y=\"TotalDistance\", data=da_days,palette =\"deep\")\nplot.set_xticklabels(ax.get_xticklabels(),rotation=90)\nplot.set_title('Total Distance During Each Day')\nax.set_ylabel('Total Distance Sum')\nax.set_xlabel('Days')\n\nTotal Distance by Month\n\nplt.figure(figsize=(8, 6))\ntotal_activity_by_month = daily_activity.groupby('Month')['TotalDistance'].sum().reset_index()\nsns.barplot(x='Month', y='TotalDistance', data=daily_activity)\nplt.xlabel('Month')\nplt.ylabel('Total Distance')\nplt.title('Total Distance by Month')\nplt.show()\n\nTotal Steps and Total Distance by Day of Week\n\n# Create a figure and the primary y-axis\nfig, ax1 = plt.subplots(figsize=(10, 6))\nsns.barplot(x='Day_of_week', y='TotalSteps', data=daily_activity, label='Total Steps', color='skyblue', ax=ax1)\n\n# Calculate a scaling factor for Total Distance\ntotal_distance_max = daily_activity['TotalDistance'].max()\nscaling_factor = daily_activity['TotalSteps'].max() / total_distance_max\n\n# Plot Total Distance (scaled) on the secondary y-axis\ndaily_activity['TotalDistance_scaled'] = daily_activity['TotalDistance'] * scaling_factor\nsns.barplot(x='Day_of_week', y='TotalDistance_scaled', data=daily_activity, label='Total Distance', color='salmon', ax=ax1)\n\n# Customize the plot\nplt.title('Total Steps and Total Distance by Day of Week')\nax1.set_xlabel('Day of Week')\nax1.set_ylabel('Count')\nplt.grid(True)\n\n# Show the plot\nplt.tight_layout()\nplt.show()\n\nTotal Steps and Total Distance by Month\n\n# Create a figure and the primary y-axis\nfig, ax1 = plt.subplots(figsize=(10, 6))\nsns.barplot(x='Month', y='TotalSteps', data=daily_activity, label='Total Steps', color='skyblue', ax=ax1)\n\n# Calculate a scaling factor for Total Distance\ntotal_distance_max = daily_activity['TotalDistance'].max()\nscaling_factor = daily_activity['TotalSteps'].max() / total_distance_max\n\n# Plot Total Distance (scaled) on the secondary y-axis\ndaily_activity['TotalDistance_scaled'] = daily_activity['TotalDistance'] * scaling_factor\nsns.barplot(x='Month', y='TotalDistance_scaled', data=daily_activity, label='Total Distance', color='salmon', ax=ax1)\n\n# Customize the plot\nplt.title('Total Steps and Total Distance by Months')\nax1.set_xlabel('Month')\nax1.set_ylabel('Count')\nplt.grid(True)\n\n# Show the plot\nplt.tight_layout()\nplt.show()\n\nAverage Activity Minutes by Day of Week\n\n# Calculate the average values for each activity type by Day_of_week\naverage_activity = daily_activity.groupby('Day_of_week').agg({\n    'VeryActiveMinutes': 'mean',\n    'FairlyActiveMinutes': 'mean',\n    'LightlyActiveMinutes': 'mean',\n    'SedentaryMinutes': 'mean'\n}).reset_index()\n\n# Melt the DataFrame to create a tidy format for plotting\nmelted_df = pd.melt(average_activity, id_vars='Day_of_week', var_name='Activity', value_name='Minutes')\n\n# Create a scatter plot with linear regression line\nplt.figure(figsize=(10, 6))\nsns.lmplot(x='Day_of_week', y='Minutes', hue='Activity', data=melted_df, height=6, aspect=1.5)\n\n# Customize the plot\nplt.title('Average Activity Minutes by Day of Week')\nplt.xlabel('Day of Week')\nplt.ylabel('Average Minutes')\nplt.legend(title='Activity Type')\n\n# Show the plot\nplt.tight_layout()\nplt.show()\n\nTotal Calories During Each Day\n\ndc_days = daily_calories.groupby('Day_of_week').agg({'Calories':'sum'}).reset_index().sort_values('Calories',ascending = False)\n\na1 = (8, 6)\nfig, ax = plt.subplots(figsize=a1)\nplot= sns.barplot(x=\"Day_of_week\", y=\"Calories\", data=dc_days,palette =\"deep\")\nplot.set_xticklabels(ax.get_xticklabels(),rotation=90)\nplot.set_title('Total Calories During Each Day')\nax.set_ylabel('Total Calories')\nax.set_xlabel('Days')\n\nTotal Calories by Month\n\nplt.figure(figsize=(8, 6))\ntotal_calories_by_month = daily_calories.groupby('Month')['Calories'].sum().reset_index()\nsns.barplot(x='Month', y='Calories', data=daily_calories)\nplt.xlabel('Month')\nplt.ylabel('Total Calories')\nplt.title('Total Calories by Month')\nplt.show()\n\nTotal Minutes A Sleep During Each Day\n\nsd_days = sleep_day.groupby('Day_of_week').agg({'TotalMinutesAsleep':'sum'}).reset_index().sort_values('TotalMinutesAsleep',ascending = False)\n\na1 = (8, 6)\nfig, ax = plt.subplots(figsize=a1)\nplot= sns.barplot(x=\"Day_of_week\", y=\"TotalMinutesAsleep\", data=sd_days,palette =\"deep\")\nplot.set_xticklabels(ax.get_xticklabels(),rotation=90)\nplot.set_title('Total Minutes A Sleep During Each Day')\nax.set_ylabel('Total Minutes A Sleep')\nax.set_xlabel('Days')\n\nTotal Minutes A Sleep by Month\n\nplt.figure(figsize=(8, 6))\ntotal_sleep_day_by_month = sleep_day.groupby('Month')['TotalMinutesAsleep'].sum().reset_index()\nsns.barplot(x='Month', y='TotalMinutesAsleep', data=sleep_day)\nplt.xlabel('Month')\nplt.ylabel('Total Minutes A Sleep')\nplt.title('Total Minutes A Sleep by Month')\nplt.show()\n\nTotal WeightKg During Each Day\n\nwl_info = weight_log_info.groupby('Day_of_week').agg({'WeightKg':'sum'}).reset_index().sort_values('WeightKg',ascending = False)\n\na1 = (8, 6)\nfig, ax = plt.subplots(figsize=a1)\nplot= sns.barplot(x=\"Day_of_week\", y=\"WeightKg\", data=wl_info,palette =\"deep\")\nplot.set_xticklabels(ax.get_xticklabels(),rotation=90)\nplot.set_title('Total WeightKg During Each Day')\nax.set_ylabel('Total WeightKg')\nax.set_xlabel('Days')\n\nWeight Distributions (Kg)\n\n# Histogram for WeightKg\nplt.figure(figsize=(10, 6))\nsns.histplot(data=weight_log_info, x='WeightKg', bins=20, kde=True)\nplt.title('Weight Distribution (Kg)')\nplt.xlabel('Weight (Kg)')\nplt.ylabel('Frequency')\nplt.show()\n\nWeight (Pounds) by Report Type\n\n# Create a violin plot to compare WeightPounds for manual and automatic reports\nplt.figure(figsize=(8, 6))\nsns.violinplot(data=weight_log_info, x='IsManualReport', y='WeightPounds')\nplt.title('Weight (Pounds) by Report Type')\nplt.xlabel('Report Type (Manual or Automatic)')\nplt.ylabel('Weight (Pounds)')\nplt.show()\n\nVisualizations and Analysis\nHeatmaps Correlations\nA correlation heatmap is a graphical representation of a correlation matrix representing the correlation between different variables. The value of correlation can take any value from -1 to 1. Correlation between two random variables or bivariate data does not necessarily imply a causal relationship. We can see it as in Fig. 3 below ðŸ‘‡\nFig. 3. Heatmaps Correlations.Very Active Minutes by Day of the Week\nText\nFig. 4. Very Active Minutes by Day of the Week.Text\nSedentary Minutes by Day of the Week\nText\nFig. 5. Sedentary Minutes by Day of the WeekText\nTotal Steps by Day of the Week\nText\nFig. 6. Total Steps by Day of the WeekText\nRelationship between Total Steps and Calories\nText\nFig. 7. Relationship between Total Steps and CaloriesText\nTotal Distance During Each day\nText\nFig. 8. Total Distance During Each dayText\nTotal Distance by Month\nText\nFig. 9. Total Distance by MonthText\nTotal Steps and Total Distance by Day of Week\nText\nFig. 10. Total Steps and Total Distance by Day of WeekText\nTotal Steps and Total Distance by Month\nText\nFig. 11. Total Steps and Total Distance by MonthText\nAverage Activity Minutes by Day of Week\nText\nFig. 12. Average Activity Minutes by Day of WeekText\nTotal Calories During Each Day\nText\nFig. 13. Total Calories During Each DayText\nTotal Calories by Month\nText\nFig. 14. Total Calories by MonthText\nTotal Minutes A Sleep During Each Day\nText\nFig. 15. Total Minutes A Sleep During Each DayText\nTotal Minutes A Sleep by Month\nText\nFig. 16. Total Minutes A Sleep by MonthText\nTotal WeightKg During Each Day\nText\nFig. 17. Total WeightKg During Each DayText\nWeight Distributions (Kg)\nText\nFig. 18. Weight Distributions (Kg)Text\nWeight (Pounds) by Report Type\nText\nFig. 19. Weight (Pounds) by Report TypeText\n\n\n\n",
    "preview": "posts/2023-10-03-utilizing-health-tech-device-usage-trends-to-inform-marketing-strategy-bellabeat-analysis/bellabeat.jpeg",
    "last_modified": "2023-10-03T12:26:27+07:00",
    "input_file": "utilizing-health-tech-device-usage-trends-to-inform-marketing-strategy-bellabeat-analysis.knit.md"
  },
  {
    "path": "posts/2023-09-24-timesheet-monitoring-tracker/",
    "title": "TimeSheet Monitoring Tracker",
    "description": "A TimeSheet Monitoring Tracker to track the working hours performed by the intern employees.",
    "author": [
      {
        "name": "Agus Santoso",
        "url": {}
      }
    ],
    "date": "2023-09-24",
    "categories": [
      "Spreadsheets",
      "Monitoring Tracker",
      "Data Visualizations"
    ],
    "contents": "\nIntroductions\nThis project is undertaken due to the people teamâ€™s and mentor need to track the working hours performed by the intern employees during their work activities on a weekly basis. This is to observe the performance patterns of the interns.\nWhat Tools To Use\nSpreadsheets\nSCQA Framework\nHereâ€™s the SCQA Frameworks we use:\nSituation:\nThere is a working hour regulation for interns, which is currently set at 40 hours per week.\nThere is a discussion about changing the working hour regulation with two options: 30 hours per week or 15 hours per week.\nThe Campaign has implemented a work deliverables sheet in the commitment form, which serves as evidence of data regarding the tasks and working hours of interns every day.\nComplication:\nThe implementation of work deliverables has not been optimal due to the difficulty in monitoring, both by mentors and the people team.\nFeedback from some interns indicates that it is not very effective due to excessive administrative work, such as the obligation to fill out standup.ly and Jira tasks.\nAs a result, data related to the average working hours of interns per week cannot be compiled.\nQuestion:\nHow can we monitor and track the working hours of interns along with the tasks/projects they are working on?\nAnswer:\nTime/Task/Project Sheet Tracker\nModify the existing work deliverables sheet to create a sheet/form that is visually easier for interns to fill out.\nMentors can more easily monitor and provide feedback on a weekly/monthly basis.\nWorking hour data can be pulled by the people team, ensuring that interns comply with their choice of working hours (30 or 15 hours per week).\nPreparations\nHere, I want to describe a few tools in this analysis:\nSpreadsheets\nTools/Functions we use:\nFiltering\nSlicer\nImportrange\nFormating\nIF Functions\netc\n\nFiles : I want to know you Data files are hidden due to company secrets.\nStep and Plan\nCreate Template Work Deliverables Sheet\nCreate a new template with modifications from the previous sheet. This template will later be duplicated by each intern, then the data in the work deliverables sheet will be pulled into the Time Sheet Tracker (people-team data center). Heres the picture Work Deliverables Sheet Template.\n\nFig. 1. Template Work Deliverables Sheet.\nTimeSheet Monitoring Tracker\nCreate a TimeSheet Monitoring Tracker (Central Data). In the time sheet tracker there will be 4 sections:\nGuidlines: Contains How to Use the TimeSheet Monitoring Tracker\n\nFig. 2. Guidlines Tracker\nPeople Team Monitoring: Contains a total recap of internal work deliverables such as total working hours per week per intern.\n\nFig. 3. People Team Monitoring Tracker\nMentor Monitoring: Recap of internal work deliverables per team, to make it easier for mentors to check without having to open internal deliverables one by one.\n\nFig. 4. Mentor Monitoring Tracker\nData-Viz: To see a graph of the development of internal working hours.\n\nFig. 5. Data Vizualization Monitoring Tracker\nTrial and Error\nAfter the work deliverables sheets and TimeSheet Monitoring Tracker are created, we carry out the next step, namely by Trial & Error to finding out of the best way to reach a desired result or a correct solution by trying out one or more ways or means and by noting and eliminating errors or causes of failure.\nEvaluations\nEvaluation needs to be carried out to assess or calculate the quality of the TimeSheet Monitoring Tracker that we have developed. We can do this with all the data that has been collected since TimeSheet Monitoring Tracker was used.\nFinalization\nIn the last step we finalize it after all the steps have been carried out and the TimeSheet Monitoring Tracker can be used properly.\n\n\n\n",
    "preview": "posts/2023-09-24-timesheet-monitoring-tracker/mt-peopleteam.png",
    "last_modified": "2023-09-29T15:38:04+07:00",
    "input_file": {},
    "preview_width": 1440,
    "preview_height": 900
  }
]
